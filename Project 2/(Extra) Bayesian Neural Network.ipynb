{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of notebook\n",
    "\n",
    "This notebook includes the code for the solution to task 5: Bayesian Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'corner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9d1c41eeafa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Third party packes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0memcee\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcorner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'corner'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Standard packages\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "import sys\n",
    "\n",
    "# Third party packes\n",
    "import emcee\n",
    "import corner\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Custom imports\n",
    "from lattice import Lattice\n",
    "\n",
    "# Set numpy seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot params\n",
    "plt.rc('font', size=14)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=14)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=16)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for generating data - from project description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 x 10 lattice\n",
    "# 60 temperatures, 500 thermalization iterations\n",
    "\n",
    "# For a temperature range, thermalize a lattice, then\n",
    "# take a few hundred steps, recording energy and magnetization.\n",
    "# Store the means to plot next.\n",
    "# This takes about 60s with one modern core.\n",
    "\n",
    "# Thermalization and measurement steps\n",
    "ntherm = 500\n",
    "nmeasure = 200\n",
    "\n",
    "# points = array with (T, mean(E), abs(mean(M)), var(E))\n",
    "# with the mean and variance evaluated for a list of many temperatures\n",
    "points = []\n",
    "# Storing nmeasure / nsparse data points\n",
    "nsparse = 10\n",
    "# points_full = array with (T, E, abs(M))\n",
    "# for several different configurations per temperature\n",
    "points_full=[]\n",
    "for T in tqdm(np.arange(4.0,1.0,-0.05)):\n",
    "    lat = Lattice(N=10,T=T)\n",
    "    for _ in range(ntherm):\n",
    "        lat.step()\n",
    "    Es = []\n",
    "    Ms = []\n",
    "\n",
    "    for istep in range(nmeasure): \n",
    "        lat.step()\n",
    "        Es.append(lat.get_energy())\n",
    "        Ms.append(lat.get_avg_magnetization())\n",
    "        if (istep%nsparse==0):\n",
    "            points_full.append((T,Es[-1],np.abs(Ms[-1])))           \n",
    "    Es = np.array(Es)\n",
    "    Ms = np.array(Ms)\n",
    "    points.append((T,Es.mean(),np.abs(Ms.mean()),Es.var()))\n",
    "points = np.array(points)\n",
    "points_full = np.array(points_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate critical temperature\n",
    "Tc = 2 / np.log(1+np.sqrt(2))\n",
    "print(f\"Critical temperature: Tc = {Tc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for BNN\n",
    "\n",
    "Prepare the data for the BNN by only using low and high temperature data and normalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data. Remove mean and divide with standard deviation\n",
    "def normalize(train_data, test_data):\n",
    "    '''\n",
    "    Normalize train_data and test_data using the mean and standard deviation from the \n",
    "    train_data set. This is as to not contaminate the test_data and introduce correlations. \n",
    "    '''\n",
    "    # Iterate over all columns in training data, and normalize all data using training data!\n",
    "    mean_save = np.zeros(2) # To save for renormalization in model evaluation (See contour plot)\n",
    "    std_save = np.zeros(2)\n",
    "    for i in range(1,2+1):\n",
    "        # We want to normalize columns 1 and 2 - not T, will be one-hot encoded!\n",
    "        mean = train_data[:,i].mean(axis=0)\n",
    "        std = train_data[:,i].std(axis=0)\n",
    "        assert (std-0) > 1e-5  # Check so standard deviation is not zero!\n",
    "        train_data[:,i] -= mean\n",
    "        train_data[:,i] /= std\n",
    "        test_data[:,i] -= mean\n",
    "        test_data[:,i] /= std\n",
    "        mean_save[i-1] = mean\n",
    "        std_save[i-1] = std\n",
    "    return train_data, test_data, mean_save, std_save\n",
    "\n",
    "\n",
    "def data_split(data, train_fraction, seed, shuffle):\n",
    "    '''Shuffle the dataset, and then split into two fractions'''\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)  # Shuffles along the first dimension, i.e. along the row dimension. \n",
    "    # Split into two parts, with the train_set being train_frac\n",
    "    split_idx = int(data.shape[0]*train_fraction)\n",
    "    test_data = data[:split_idx]\n",
    "    train_data = data[split_idx:]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def one_hot_labels(data, Tc):\n",
    "    '''One hot encodes the first column in data. Use boolean expressions.'''\n",
    "    bool_arr = data[:,0] > Tc\n",
    "    data[:,0] = bool_arr\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def prepare_data(full_data, Tc, test_fraction=0.3, seed=1, shuffle=True, low_high_T=False, Tlo = 1.5, Thi = 3.5):\n",
    "    '''\n",
    "    One-hot encodes T-column as 0 if T < Tc and 1 if T > Tc.  \n",
    "    Shuffles and splits the dataset into a training and a test set. \n",
    "    Finally normalizes the test and train sets using the train set.\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    data = np.copy(full_data) # Make a copy, as not to overwrite original data\n",
    "    if low_high_T:\n",
    "        # Only pick out the datarows at the edges of the temperature range\n",
    "        idx_hi = data[:,0] >= Thi \n",
    "        idx_lo = data[:,0] <= Tlo\n",
    "        idx = idx_hi + idx_lo\n",
    "        data =  data[idx]\n",
    "    data = one_hot_labels(data, Tc)  # One-hot encode labels\n",
    "    train_data, test_data = data_split(data, test_fraction, seed, shuffle)  # Shuffle and split the dataset\n",
    "    train_data, test_data, mean, std = normalize(train_data, test_data)\n",
    "    \n",
    "    return train_data, test_data, mean, std\n",
    "    \n",
    "train_data, test_data, mean, std = prepare_data(points_full, Tc, 0.3, seed=1, low_high_T=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the BNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in $p(y^{N+1}|x^{N+1}, D)$. Using marginalization, the product rule and Baye's theorem, we can write this as: \n",
    "\n",
    "$$ p(y^{N+1}|x^{N+1}, D) = \\int_W dW p(y^{N+1}|x^{N+1}, D, W) p(W|D).$$\n",
    "\n",
    "$p(y^{N+1}|x^{N+1}, D, W)$  is often approximated as $y(x^{N+1}; W)$ (?) since that is really no pdf for a fix value of $W$. $p(W|D)$ is given as:\n",
    "\n",
    "$$ p(W|D) \\propto p(D|W) p(W),$$\n",
    "\n",
    "which is the likelihood for our data and our prior for our weights $ W $ respectively. Thus we have, in total: \n",
    "\n",
    "$$ p(y^{N+1}|x^{N+1}, D) \\propto \\int_W dW y(x^{N+1}; W) p(D|W) p(W),$$\n",
    "\n",
    "where we can sample the integral using MCMC sampling over our $W$-space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define priors for our weights and bias, which we are assume to be independent, as well as our log likelihood, log posterior and feed forward pass (from Task 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior for W and b\n",
    "def log_prior(W, sigma=1.0):\n",
    "    '''\n",
    "    Gaussian prior centered around 0 for weights W (including bias b=W[0]). The total \n",
    "    prior is the product of the priors for b, W0 and W1, all with the same sigma. \n",
    "    '''\n",
    "    return -W.shape[0]/2*np.log(2*np.pi*sigma**2) - (W.dot(W))/sigma**2\n",
    "\n",
    "\n",
    "# TODO Define likelihood for data D given W\n",
    "def log_likelihood(W, t, x):\n",
    "    '''Log-binary crossentropy'''\n",
    "    y = feed_forward(x, W[1:], W[0])\n",
    "    \n",
    "    return np.sum(t*np.log(y) + (1-t)*np.log(1-y))\n",
    "\n",
    "\n",
    "def feed_forward(x, W, b):\n",
    "    '''Performs the feed-forward pass for our neuron. Return the activation.'''\n",
    "    a = x@W + b\n",
    "    \n",
    "    return sigmoid(a)  \n",
    "\n",
    "\n",
    "def sigmoid(a):\n",
    "    '''¨Returns the sigmoid value of the activation a.'''\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "\n",
    "def log_posterior(W, t, x, sigma):\n",
    "    '''Returns the log posterior, given weights W, targets t, training data t and weight decay sigma.'''\n",
    "    return log_prior(W, sigma) + log_likelihood(W, t, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the weight space with emcee\n",
    "\n",
    "Use ten walkers to sample the three weight and bias dimensions. This cell takes a few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling properties\n",
    "ndim = 3\n",
    "nwalkers = 10\n",
    "W0 = np.random.randn(nwalkers, ndim)\n",
    "nburn = 200\n",
    "nsamples = 10000\n",
    "\n",
    "# Additional arguments for the sampler e.g. target and training data\n",
    "argtuple = (train_data[:,0], train_data[:,1:], 1)  # targets, input data, sigma\n",
    "\n",
    "# Define sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=argtuple)\n",
    "\n",
    "# Perform the sampling\n",
    "t0 = time.time()\n",
    "sampler.run_mcmc(W0, nburn+nsamples)\n",
    "t1 = time.time()\n",
    "print(f'Time for sample: {(t1-t0):.1f} seconds')\n",
    "\n",
    "# Extract the samples and ignore the burn-in\n",
    "samples = sampler.chain[:, nburn:, :].reshape(-1, ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract weights and bias pdf:s for the BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the weights with corner\n",
    "fig_weights = corner.corner(samples, labels=[r'$b$', r'$W_1$', r'$W_2$'], label_kwargs={'fontsize': 20},\n",
    "                            show_titles=True, title_kwargs={'fontsize': 17})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict for high, low and near critical temperatures using the BNN\n",
    "\n",
    "Plot the predictions over all our sampled models as histograms. Do this for high, low and near critical temperature data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all weights and biases for all of our models\n",
    "bias = samples[:,0].reshape(1,-1) # Apply transpose to achieve correct shape for feed forward\n",
    "weights = samples[:,1:].T\n",
    "x_train = test_data[:,1:]\n",
    "y_pred = feed_forward(x_train, weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram over all models for one sample datapoint\n",
    "high_T_p = points_full[points_full[:,0] > 3.5][0]\n",
    "low_T_p = points_full[points_full[:,0] < 1.5][-1]\n",
    "critical_T_p = points_full[np.abs(points_full[:,0] - Tc) < 0.02][0]\n",
    "\n",
    "print(f'High temperature: {high_T_p[0]:.2f} K.')\n",
    "print(f'Low temperature: {low_T_p[0]:.2f} K.')\n",
    "print(f'Close to critical temperature: {critical_T_p[0]:.2f} K.')\n",
    "\n",
    "# Normalize\n",
    "high_T_norm = np.array((high_T_p[1:] - mean)/std).reshape(1,-1)\n",
    "low_T_norm = (low_T_p[1:] - mean)/std\n",
    "critical_T_norm = (critical_T_p[1:] - mean)/std\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,6))\n",
    "\n",
    "# High temperaure\n",
    "y_high = feed_forward(high_T_norm, weights, bias)\n",
    "\n",
    "sample_idx = 100\n",
    "ax[0].hist(y_high[0], bins=50, color='r', label='Model predictions', density=True)\n",
    "ax[0].axvline(y_high.mean(), c='k', linestyle='--', linewidth=3, label='Model average')\n",
    "ax[0].set_xlabel(r'$p(T=1)$')\n",
    "ax[0].set_ylabel('Sampled probability density')\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].grid()\n",
    "\n",
    "\n",
    "# Low temperature\n",
    "y_low = feed_forward(low_T_norm, weights, bias)\n",
    "sample_idx = 100\n",
    "ax[1].hist(y_low[0], bins=50, color='b',label='Model predictions', density=True)\n",
    "ax[1].axvline(y_low.mean(), c='k', linestyle='--', linewidth=3, label='Model average')\n",
    "ax[1].set_xlabel(r'$p(T=1)$')\n",
    "ax[1].set_ylabel('Sampled probability density')\n",
    "ax[1].legend(loc='best')\n",
    "ax[1].grid()\n",
    "\n",
    "# Close to critical temperature\n",
    "y_crit = feed_forward(critical_T_norm, weights, bias)\n",
    "sample_idx = 100\n",
    "ax[2].hist(y_crit[0], bins=50, color='g', label='Model predictions', density=True)\n",
    "ax[2].axvline(y_crit.mean(), c='k', linestyle='--', linewidth=3, label='Model average')\n",
    "ax[2].set_xlabel(r'$p(T=1)$')\n",
    "ax[2].set_ylabel('Sampled probability density')\n",
    "ax[2].legend(loc='best')\n",
    "ax[2].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Generate contour plots for mean prediction and uncertainty as measured by standard deviation\n",
    "\n",
    "Mean and standard deviation are computed over all our sampled models, evaluated on a $E$, $m$ grid. The contours descibes the output of the BNN in a bit more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define endpoints of grid\n",
    "E_max = points_full[:,1].max()\n",
    "E_min = points_full[:,1].min()\n",
    "m_max = points_full[:,2].max()\n",
    "m_min = points_full[:,2].min()\n",
    "\n",
    "# Define grid\n",
    "step_E = 2\n",
    "step_m = 0.01\n",
    "E = np.arange(E_min, E_max+step_E, step_E)\n",
    "m = np.arange(m_min, m_max+step_m, step_m)\n",
    "\n",
    "# Normalize data with same mean and std as for training data\n",
    "E_norm = (E - mean[0]) / std[0]\n",
    "m_norm = (m - mean[1]) / std[1]\n",
    "\n",
    "# Calculate predictions. Apply for-loops for simplicity due to low number of samples\n",
    "pred_mean = np.zeros((m_norm.shape[0], E_norm.shape[0]))\n",
    "pred_std = np.zeros((m_norm.shape[0], E_norm.shape[0]))\n",
    "for i, En in enumerate(tqdm(E_norm)):\n",
    "    for j, mn in enumerate(m_norm):\n",
    "        x = np.array([En, mn]).reshape(1,2)\n",
    "        y = feed_forward(x, weights, bias)\n",
    "        pred_mean[j,i] = np.mean(y, axis=1)\n",
    "        pred_std[j,i] = np.std(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the mean contour\n",
    "high_T = points_full[:,0]>Tc\n",
    "low_T = points_full[:,0]<Tc\n",
    "E_M_high = points_full[high_T][:,1:]\n",
    "E_M_low = points_full[low_T][:,1:]\n",
    "\n",
    "cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "contour = ax.contourf(E, m, pred_mean, cmap=cmap, alpha=0.6)\n",
    "ax.scatter(E_M_high[:,0],E_M_high[:,1],c='r', label=r'$T>T_c$')\n",
    "ax.scatter(E_M_low[:,0],E_M_low[:,1],c='b', label=r'$T<T_c$')\n",
    "\n",
    "# Figure properties\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set_xlim(E_min, E_max)\n",
    "ax.set_ylim(m_min, m_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the standard deviation contour\n",
    "cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "contour = ax.contourf(E, m, pred_std, cmap=cmap, alpha=0.6)\n",
    "ax.scatter(E_M_high[:,0],E_M_high[:,1],c='r', label=r'$T>T_c$')\n",
    "ax.scatter(E_M_low[:,0],E_M_low[:,1],c='b', label=r'$T<T_c$')\n",
    "\n",
    "# Figure properties\n",
    "cbar = plt.colorbar(contour, ax=ax)\n",
    "ax.set_xlim(E_min, E_max)\n",
    "ax.set_ylim(m_min, m_max)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
