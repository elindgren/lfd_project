{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Custom imports\n",
    "from lattice import Lattice\n",
    "\n",
    "# Set plot params\n",
    "plt.rc('font', size=14)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=14)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=16)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=14)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=14)    # legend fontsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 60/60 [00:31<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# 10 x 10 lattice\n",
    "# 60 temperatures, 500 thermalization iterations\n",
    "\n",
    "# For a temperature range, thermalize a lattice, then\n",
    "# take a few hundred steps, recording energy and magnetization.\n",
    "# Store the means to plot next.\n",
    "# This takes about 60s with one modern core.\n",
    "\n",
    "# Thermalization and measurement steps\n",
    "ntherm = 500\n",
    "nmeasure = 200\n",
    "\n",
    "# points = array with (T, mean(E), abs(mean(M)), var(E))\n",
    "# with the mean and variance evaluated for a list of many temperatures\n",
    "points = []\n",
    "# Storing nmeasure / nsparse data points\n",
    "nsparse = 10\n",
    "# points_full = array with (T, E, abs(M))\n",
    "# for several different configurations per temperature\n",
    "points_full=[]\n",
    "for T in tqdm(np.arange(4.0,1.0,-0.05)):\n",
    "    lat = Lattice(N=10,T=T)\n",
    "    for _ in range(ntherm):\n",
    "        lat.step()\n",
    "    Es = []\n",
    "    Ms = []\n",
    "\n",
    "    for istep in range(nmeasure): \n",
    "        lat.step()\n",
    "        Es.append(lat.get_energy())\n",
    "        Ms.append(lat.get_avg_magnetization())\n",
    "        if (istep%nsparse==0):\n",
    "            points_full.append((T,Es[-1],np.abs(Ms[-1])))           \n",
    "    Es = np.array(Es)\n",
    "    Ms = np.array(Ms)\n",
    "    points.append((T,Es.mean(),np.abs(Ms.mean()),Es.var()))\n",
    "points = np.array(points)\n",
    "points_full = np.array(points_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical temperature: Tc = 2.2692\n"
     ]
    }
   ],
   "source": [
    "# Calculate critical temperature\n",
    "Tc = 2 / np.log(1+np.sqrt(2))\n",
    "print(f\"Critical temperature: Tc = {Tc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 3)\n",
      "(400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Normalize data. Remove mean and divide with standard deviation\n",
    "def normalize(train_data, test_data):\n",
    "    '''\n",
    "    Normalize train_data and test_data using the mean and standard deviation from the \n",
    "    train_data set. This is as to not contaminate the test_data and introduce correlations. \n",
    "    '''\n",
    "    # Iterate over all columns in training data, and normalize all data using training data!\n",
    "    for i in range(1,2+1):\n",
    "        # We want to normalize columns 1 and 2 - not T, will be one-hot encoded!\n",
    "        mean = train_data[:,i].mean(axis=0)\n",
    "        std = train_data[:,i].std(axis=0)\n",
    "        assert (std-0) > 1e-5  # Check so standard deviation is not zero!\n",
    "        train_data[:,i] -= mean\n",
    "        train_data[:,i] /= std\n",
    "        test_data[:,i] -= mean\n",
    "        test_data[:,i] /= std\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def data_split(data, train_fraction, seed, shuffle):\n",
    "    '''Shuffle the dataset, and then split into two fractions'''\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data)  # Shuffles along the first dimension, i.e. along the row dimension. \n",
    "    # Split into two parts, with the train_set being train_frac\n",
    "    split_idx = int(data.shape[0]*train_fraction)\n",
    "    test_data = data[:split_idx]\n",
    "    train_data = data[split_idx:]\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def one_hot_labels(data, Tc):\n",
    "    '''One hot encodes the first column in data. Use boolean expressions.'''\n",
    "    bool_arr = data[:,0] > Tc\n",
    "    data[:,0] = bool_arr\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def prepare_data(full_data, Tc, test_fraction=0.3, seed=1, shuffle=True, low_high_T=False, Tlo = 1.5, Thi = 3.5):\n",
    "    '''\n",
    "    One-hot encodes T-column as 0 if T < Tc and 1 if T > Tc.  \n",
    "    Shuffles and splits the dataset into a training and a test set. \n",
    "    Finally normalizes the test and train sets using the train set.\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    data = np.copy(full_data) # Make a copy, as not to overwrite original data\n",
    "    if low_high_T:\n",
    "        # Only pick out the datarows at the edges of the temperature range\n",
    "        idx_hi = data[:,0] >= Thi \n",
    "        idx_lo = data[:,0] <= Tlo\n",
    "        idx = idx_hi + idx_lo\n",
    "        data =  data[idx]\n",
    "    data = one_hot_labels(data, Tc)  # One-hot encode labels\n",
    "    train_data, test_data = data_split(data, test_fraction, seed, shuffle)  # Shuffle and split the dataset\n",
    "    train_data, test_data = normalize(train_data, test_data)\n",
    "    \n",
    "    return(train_data, test_data)\n",
    "    \n",
    "train_data, test_data = prepare_data(points_full, Tc, 0.3, seed=1, low_high_T=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define BNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in $p(y^{N+1}|x^{N+1}, D)$. Using marginalization, the product rule and Baye's theorem, we can write this as: \n",
    "\n",
    "$$ p(y^{N+1}|x^{N+1}, D) = \\int_W dW p(y^{N+1}|x^{N+1}, D, W) p(W|D).$$\n",
    "\n",
    "$p(y^{N+1}|x^{N+1}, D, W)$  is often approximated as $y(x^{N+1}; W)$ (?) since that is really no pdf for a fix value of $W$. $p(W|D)$ is given as:\n",
    "\n",
    "$$ p(W|D) \\propto p(D|W) p(W),$$\n",
    "\n",
    "which is the likelihood for our data and our prior for our weights $ W $ respectively. Thus we have, in total: \n",
    "\n",
    "$$ p(y^{N+1}|x^{N+1}, D) \\propto \\int_W dW y(x^{N+1}; W) p(D|W) p(W),$$\n",
    "\n",
    "where we can sample the integral using MCMC sampling over our $W$-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-29-e4ab932a7911>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-29-e4ab932a7911>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    return 1/np.sqrt(2*np.pi*sigma**2)np.exp(-(W.dot(W)+b**2)/sigma**2)\u001b[0m\n\u001b[1;37m                                       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# We need the feed forward pass here again - save that from Task 1. \n",
    "\n",
    "# Define prior for W and b\n",
    "def prior_w_and_b(W, b, sigma=1):\n",
    "    '''\n",
    "    Define a gaussian prior centered around 0 for weights W and bias b. The total \n",
    "    prior is the product of the priors for b, W0 and W1, all with the same sigma. \n",
    "    '''\n",
    "    return 1/np.sqrt(2*np.pi*sigma**2)np.exp(-(W.dot(W)+b**2)/sigma**2)\n",
    "\n",
    "# TODO Define likelihood for data D given W\n",
    "def likelihood(D, W, b):\n",
    "    '''Will here be the crossentropy?'''\n",
    "    return None\n",
    "\n",
    "# Define feed-forward pass given data x^(n+1) and weights W.\n",
    "def feed_forward(x, W, b):\n",
    "    '''Performs the feed-forward pass for our neuron. Return the activation.'''\n",
    "    z = x@W + b\n",
    "    # return sigmoid(z)  \n",
    "    return z\n",
    "   \n",
    "\n",
    "\n",
    "# TODO MCMC sample the integrand and marginalize out all our W parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract weights and bias pdf:s for the BNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the BNN for high, low and near critical temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
