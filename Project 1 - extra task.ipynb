{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emcee \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task c: Reproduce Table III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to sample our parameter space for our features $\\{a\\}$, for different values of $k$ and $k_{max}$, using both our uniform and naturaleness priors as defined in the basic tasks. However, the main difference is that we need to marginalize out the extra parameters for higher values of $k_{max}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a modular model that generates features up to k_max: however we \n",
    "# then marginalize all larger k. \n",
    "def modular_model(a, x, kmax):\n",
    "    '''\n",
    "    Returns model of order kmax. a is a vector containing the model features.\n",
    "    Note the kmax+1 in the for loop. This is due to range being from 0 to\n",
    "    kmax-1.\n",
    "    '''\n",
    "    model = 0\n",
    "    for k in range(kmax+1):\n",
    "        model += a[k]*x**k\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\chi^2/\\text{dof}$ means the value of $\\chi^2$ per degree of freedom. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy most of the code from the basic part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': array([0.03183, 0.06366, 0.09549, 0.12732, 0.15915, 0.19099, 0.22282,\n",
       "        0.25465, 0.28648, 0.31831]),\n",
       " 'd': array([0.31694, 0.33844, 0.42142, 0.57709, 0.56218, 0.68851, 0.73625,\n",
       "        0.8727 , 1.0015 , 1.0684 ]),\n",
       " 'sigma': array([0.01585 , 0.01692 , 0.02107 , 0.02885 , 0.02811 , 0.03443 ,\n",
       "        0.03681 , 0.04364 , 0.050075, 0.05342 ])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "def load_data(file):\n",
    "    d = {\n",
    "        \"x\": [],\n",
    "        \"d\": [],\n",
    "        \"sigma\": []\n",
    "    }\n",
    "    # Skip first two rows, which are the header:\n",
    "    with open(file) as f:\n",
    "        for idx,line in enumerate(f):\n",
    "            if idx < 3:\n",
    "                pass\n",
    "            else:\n",
    "                val = line.split()\n",
    "                d[\"x\"].append(np.float(val[0]))\n",
    "                d[\"d\"].append(np.float(val[1]))\n",
    "                d[\"sigma\"].append(np.float(val[2]))\n",
    "    # cast to numpy arrays\n",
    "    d[\"x\"] = np.array(d[\"x\"])\n",
    "    d[\"d\"] = np.array(d[\"d\"])\n",
    "    d[\"sigma\"] = np.array(d[\"sigma\"])\n",
    "    return d\n",
    "\n",
    "\n",
    "file = 'D1_c_5.dat'\n",
    "data = load_data(file)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def log_uniform_prior(a):\n",
    "    '''\n",
    "    Uniform prior, returns a log(1) if the values in a are in abs(a)<100. Note that this\n",
    "    prior is not normalized. We take care of this later.\n",
    "    '''\n",
    "    if np.all(np.abs(a)<=100):\n",
    "        return 0  # log(1)\n",
    "    else: \n",
    "        return -np.inf  # log(0)\n",
    "    \n",
    "\n",
    "def log_naturaleness_prior(a, bar_a=5):\n",
    "    '''Naturaleness prior implemented according to equation 24 with bar(a)=5. This ensures'''\n",
    "    return -len(a)*np.log(np.sqrt(2*np.pi)*bar_a) - 1/2*(a.dot(a)/bar_a**2)\n",
    "\n",
    "# Tests \n",
    "print(0 == log_uniform_prior([-1,2,50]))\n",
    "print(-np.inf == log_uniform_prior([-1,2,500]))\n",
    "print(-7.8651 == np.round(np.log((1/(np.sqrt(2*np.pi)*5))**3 * np.exp(-(1+4+9)/(2*5**2))),4) == np.round(log_naturaleness_prior(np.array([1,2,3]), 5),4))  # Calculate expression exact and log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: [ True  True]\n",
      "chi_squared: True\n",
      "log likelihood: True\n"
     ]
    }
   ],
   "source": [
    "def chi_squared(a, d, x, sigmas, kmax):\n",
    "    '''\n",
    "    Returns the chi squared measure for the datapoints d and x. The standard deviation is \n",
    "    assumed to be constant for all datapoints.\n",
    "    '''\n",
    "    chi_vec = (d-modular_model(a, x, kmax))/sigmas\n",
    "    return np.sum(chi_vec**2)\n",
    "\n",
    "\n",
    "def log_likelihood(a, d, x, sigmas, kmax):\n",
    "    '''\n",
    "    Returns log likelihood based on a Gaussian with di as the center values and \n",
    "    a standard deviation of sigma. a is the feature vector for our model.\n",
    "    '''\n",
    "    chi_sq = chi_squared(a, d, x, sigmas, kmax)\n",
    "    like = -np.sum(np.log(np.sqrt(2*np.pi)*sigmas)) - 1/2*chi_sq\n",
    "    return like\n",
    "\n",
    "\n",
    "# Tests\n",
    "a = np.array([1,2,3,4])\n",
    "x = np.array([1,2])\n",
    "d = np.array([2,4])\n",
    "sigmas = np.array([2,4])\n",
    "kmax = len(a)-1  # Sanity check that the model still works in the basic task case\n",
    "print(f'Model: {[10, 49] == modular_model(a,x, kmax)}')\n",
    "print(f'chi_squared: {142.5625==chi_squared(a,d,x,sigmas, kmax)}')\n",
    "exact_likelihood = np.prod(1/(np.sqrt(2*np.pi)*sigmas)) * np.exp(-chi_squared(a,d,x,sigmas, kmax)/2)\n",
    "print(f'log likelihood: {np.round(np.log(exact_likelihood),4) == np.round(log_likelihood(a, d, x, sigmas, kmax),4)}')  # exact calculation, and then log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok our new model passes this sanity check. All of our tests from the basic problem returns ok with the new modular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_post_uniform(a,d,x,sigma, kmax):\n",
    "    return log_likelihood(a,d,x,sigma,kmax) + log_uniform_prior(a)\n",
    "\n",
    "\n",
    "def log_post_natural(a,d,x,sigma, kmax, bar_a):\n",
    "    return log_likelihood(a,d,x,sigma,kmax) + log_naturaleness_prior(a, bar_a=bar_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling time: 3.8116021156311035 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Sampling\n",
    "\n",
    "# Define constants and data\n",
    "bar_a = 5\n",
    "x = data[\"x\"]\n",
    "d = data[\"d\"]\n",
    "sigma = data[\"sigma\"]\n",
    "kmax = 5\n",
    "k = min(kmax,2)  # k+1 is the numbers of relevant features, so our model is at most of degree 2.\n",
    "\n",
    "# Define dimensions and walkers\n",
    "ndim = kmax+1  # 0,1,...,kmax\n",
    "nwalkers = ndim*2\n",
    "# Initial guess\n",
    "p0 = np.random.rand(ndim * nwalkers).reshape((nwalkers, ndim))\n",
    "\n",
    "\n",
    "nburn = 200  # nbr of burning steps\n",
    "nsamples = 1000  # nbr of final samples \n",
    "\n",
    "# additional arguments to our sampler: d, x, sigma and d,x,sigma, bar_a respectively\n",
    "arglist_uniform = (d, x, sigma, kmax)\n",
    "arglist_natural = (d, x, sigma, kmax, bar_a)\n",
    "\n",
    "# Define samplers\n",
    "sampler_uniform = emcee.EnsembleSampler(nwalkers, ndim, log_post_uniform, args=arglist_uniform)\n",
    "sampler_natural = emcee.EnsembleSampler(nwalkers, ndim, log_post_natural, args=arglist_natural)\n",
    "# Start sampler on posteriors. Use first few hundred iterations as burn in. \n",
    "t0 = time()  # start time\n",
    "sampler_uniform.run_mcmc(p0, nburn + nsamples)\n",
    "sampler_natural.run_mcmc(p0, nburn + nsamples)\n",
    "t1 = time()  # end time\n",
    "print(f'Sampling time: {t1-t0} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 6)\n"
     ]
    }
   ],
   "source": [
    "samples_uniform = sampler_uniform.chain[:,nburn:,:].reshape((-1,ndim))  # reshape to all samples per dim\n",
    "sampler_natural = sampler_natural.chain[:,nburn:,:].reshape((-1,ndim)) \n",
    "print(samples_uniform.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, marginalize out all features higher than order k=2, and compute the mean for our feature. We just do this by ignoring the other data corresponding to the higher features, since we are just interested in the number of times that we \"land\" on our relevant features anyway.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Uniform prior for kmax = 5 *****\n",
      "avg(a_0) = 0.2755205448297961\n",
      "avg(a_1) = 0.7459644247615889\n",
      "avg(a_2) = 10.672640275150458\n",
      "**************************************\n",
      "***** Gaussian (Natural) prior for kmax = 5 *****\n",
      "avg(a_0) = 0.2468305122930796\n",
      "avg(a_1) = 1.6919339735926673\n",
      "avg(a_2) = 2.6704201465702857\n",
      "**************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.24683051, 1.69193397, 2.67042015])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def present_feature_estimates(samples, prior_name, k, kmax):\n",
    "    print(f'***** {prior_name} prior for kmax = {kmax} *****')\n",
    "    a = np.zeros((k+1))\n",
    "    for i in range(k+1):\n",
    "        a[i] = samples[:,i].mean()\n",
    "        print(f'avg(a_{i}) = {a[i]}')\n",
    "    print('**************************************')\n",
    "    return a\n",
    "\n",
    "present_feature_estimates(samples_uniform, 'Uniform', k, kmax)\n",
    "present_feature_estimates(sampler_natural, 'Gaussian (Natural)', k, kmax)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
